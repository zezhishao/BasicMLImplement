{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Free Lunch Theorem\n",
    "\n",
    "**在没有噪声的情况下，使用错误分类率作为损失函数，假如更关系训练集外的误差，那么在所有的学习算法中没有任何先验的区分，他们都是等价的。**\n",
    "**NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么算法更好”毫无意义。因为考虑所有潜在的问题，则所哟肚饿算法都一样好。**\n",
    "\n",
    "# Ugly Ducking Theorem\n",
    "\n",
    "这个定理说的是“丑小鸭与白天鹅之间的区别和两只白天鹅之间的区别一样大”。这个看起来完全违背常识的定理实际上说的是：**世界上不存在分类的客观标准，一切分类的标准都是主观的，都依赖于人类的分类目的。**\n",
    "\n",
    "分类结果取决于选择什么特征作为分类标准，而特征的选择又依存于人的**目的**。\n",
    "\n",
    "在没有先验知识的条件下，没有更有优势甚至是最好的特征。\n",
    "\n",
    "这个先验知识来自于对问题的界定、对研究对象的直观理解。\n",
    "\n",
    "\n",
    "# Occam's Razor：奥卡姆剃刀\n",
    "\n",
    "### 如无必要，勿增实体\n",
    "\n",
    "在许多的相互竞争的假设中，我们应该选择**假设最少**的那一个。\n",
    "\n",
    "**在PRML领域中，这个问题可以被表述为：针对特定的问题，不要使用比“必要”更加复杂的模型。**\n",
    "\n",
    "**这里的“必要”的程度由能够足够好得fit训练数据来决定。**\n",
    "\n",
    "# Minimum Description Length principle\n",
    "\n",
    "**最小描述长度原则**是将[奥卡姆剃刀](https://zh.wikipedia.org/wiki/奥卡姆剃刀)形式化后的一种结果。\n",
    "\n",
    "\n",
    "MDL准则旨在寻找一些不可压缩的、最小的表示。我们应将模型的算法复杂度之和与该模型有关的训练数据的描述之和最小化。即：\n",
    "\n",
    "<img src=\"../images/K.png\" alt=\"K \" style=\"zoom:40%;\" />\n",
    "\n",
    "$K(·)$是Kolmogorov复杂度，用来衡量不可压缩性。\n",
    "\n",
    "例如神经网络中去除某些神经元之间的连接，决策树中使用熵准则剪枝。\n",
    "\n",
    "它和NFL定理并不冲突，奥卡姆剃须刀意味着，解决的问题当前的问题倾向于使用更简单的分类器。\n",
    "\n",
    "# 偏差方差窘境\n",
    "\n",
    "- Bias：模型对数据匹配的准确性\n",
    "- Variance：模型对不同问题的适应能力\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
