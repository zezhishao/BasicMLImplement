{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 回忆一下什么是似然，什么是概率：\n",
    "\n",
    ">什么是似然：给定样本 $X = x$下，参数$\\theta_1$为真实值的可能性。\n",
    ">什么是概率：在给定模型参数$\\theta_1$的情况下，样本随机变量$X=x$的可能性。\n",
    "\n",
    "### 2. 什么是EM和EM的思想\n",
    "\n",
    "在无监督分类学习中，往往面临两个问题：一是确定模型参数，二是无监督地把数据点分类。\n",
    "\n",
    "但是这两个问题又是相互纠结的，确定了一个会影响到另一个。没法单独解决，只能通过迭代的方式逐渐更新。\n",
    "\n",
    "例如$K-means$聚类问题中，一需要为数据点进行分类，二又需要计算每一个cluster的中心（即均值）。\n",
    "\n",
    "产生了一个矛盾：**为了确定分类需要确定中心；但是为了确定中心，又需要已知分类。**这意味着无法通过一次计算的方式独立地解决这两个问题。\n",
    "\n",
    "这时候就需要用EM进行逐步迭代计算。还是以$K-means$为例，既然这两个问题是相互影响的，那不如先确定一个。$K-meas$的做法，是先**随机**确定中心点，然后根据中心点给其他点分配类别。分配好了之后，根据分配结果进行计算并更新中心点。再根据新的中心点计算新的分配...如此往复直至每个点的类别分配不再改变为止。\n",
    "\n",
    "$K-meas$是一个特殊的例子，可以无视概率问题形象的解释一下EM的运算思想。\n",
    "\n",
    "而EM的的正式说法是这样的：\n",
    "\n",
    "### 3. EM的正式定义\n",
    "\n",
    "EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法。含有隐变量的概率模型的数据表示为$\\theta$ )。这里，$Y$是观测变量的数据，$Z$是隐变量的数据，$\\theta$ 是模型参数。EM算法通过迭代求解观测数据的对数似然函数${L}(\\theta)=\\log {P}(\\mathrm{Y} | \\theta)$的极大化，实现极大似然估计。每次迭代包括两步：\n",
    "\n",
    "$E$步，求期望，即求$logP\\left(Z | Y, \\theta\\right)$ )关于$ P\\left(Z | Y, \\theta^{(i)}\\right)$)的期望：\n",
    "\n",
    "$$Q\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{Z} \\log P(Y, Z | \\theta) P\\left(Z | Y, \\theta^{(i)}\\right)$$\n",
    "称为$Q$函数，这里$\\theta^{(i)}$是参数的现估计值；\n",
    "\n",
    "$M$步，求极大，即极大化$Q$函数得到参数的新估计值：\n",
    "\n",
    "$$\\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)$$\n",
    " \n",
    "在构建具体的EM算法时，重要的是定义$Q$函数。每次迭代中，EM算法通过极大化$Q$函数来增大对数似然函数${L}(\\theta)$。\n",
    "\n",
    "\n",
    "EM的这种定义是一种泛化的定义，需要根据不同的问题再设定不同的$Q$函数。\n",
    "\n",
    "EM算法在每次迭代后均提高观测数据的似然函数值，即\n",
    "\n",
    "$$P\\left(Y | \\theta^{(i+1)}\\right) \\geqslant P\\left(Y | \\theta^{(i)}\\right)$$\n",
    "\n",
    "在一般条件下EM算法是收敛的，但不能保证收敛到全局最优。\n",
    "\n",
    "### 5. EM算法的应用\n",
    "\n",
    "主要应用于含有隐变量的概率模型的学习。高斯混合模型的参数估计是EM算法的一个重要应用，隐马尔可夫模型的非监督学习也是EM算法的一个重要应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 似然函数和概率\n",
    "> 在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们运用出现的结果来判断这个事情本身的性质（参数），也就是似然。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "球袋问题、硬币问题是经典的EM问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
