{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是集成学习\n",
    "\n",
    "集成学习是一类提升方法。**“团结就是力量”**就是他的宗旨。\n",
    "\n",
    "**通过结合多个模型，可以产生一个更强大的模型。**是他的基本指导思想。\n",
    "\n",
    "集成学习是在Kaggle或者天池等比赛中，想要达到最高名次时，几乎必用的手段。最朴素的想法是投票voting。Voting是最朴素的方法。鉴于过于简单朴素，效果提升有限，用得很少。\n",
    "\n",
    "集成学习主要包括三大方法：\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Stacking\n",
    "\n",
    "🌰：\n",
    "Bagging：随机森林\n",
    "Boosting：Adaboost\n",
    "Stacking：Stacking的方法更加灵活，可以自定义层数、数据重采样方法。\n",
    "\n",
    "### 集成学习的特点\n",
    "\n",
    "集成学习的特点是**使用多个模型**，与之相伴的另一个特点是**对数据集的各种使用方法**。\n",
    "\n",
    "### 设计一个集成学习模型时需要关注的问题：\n",
    "\n",
    "1. 如何设计弱学习器\n",
    "\n",
    "> 通常这些学习器的形式是一致的\n",
    "\n",
    "2. 如何把这些弱学习器结合在一起\n",
    "\n",
    "> 结合多个学习器的算法成为Meta-Algorithms，这个算法为了产生更好的效果，调用其他算法（若分类器）作为输入。\n",
    "\n",
    "> what is meta-algorithms：\n",
    "Loosely speaking, a meta-algorithm is an algorithm that wraps and executes other algorithms and might feed them input data or use their output data. A common goal is to achieve a better task performance compared to the performance of each of those algorithms on their own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------\n",
    "\n",
    "### 1. Bagging\n",
    "\n",
    "#### 1.1 Booststrap\n",
    "\n",
    "Bagging的全称是“**B**ootstrap **AGG**regat**ING**”。\n",
    "\n",
    "Booststrap是一个重采样方法：通过从原始的$N$个样本数据$D=\\{x_1,x_2,\\dots,x_N\\}$，进行$N$次有放回采样$N$个数据 ，称为一个bootstrap样本。\n",
    "Booststrap通过**有放回的采样**，得到和原来的数据集大小不变的新数据集，其中几乎必然会有重复，所以这个操作等价于给样本reweighting。\n",
    "\n",
    "#### 1.2 Bagging的算法过程\n",
    "\n",
    "设基学习器的个数为$M$个，那么就从数据集中进行$M$次Booststrap。用这$M$个数据集训练可以得到$M$个模型。\n",
    "对这$M$个模型按如下方式投票:\n",
    "$$f_{avg}(x)=\\frac{1}{N}\\sum_{m=1}^Mf_m(x)$$\n",
    "\n",
    "**可以证明：Bagging可以降低模型的方差。**\n",
    "\n",
    "#### 1.3 Bagging具有降低模型方差的功效\n",
    "\n",
    "对于M个模型，每个模型会产生一个“样本”X，这些样本都是从一个均值为$\\mu$，方差为$\\sigma$的分布中产生的。\n",
    "\n",
    "这些样本的均值为$\\hat{X}$可以看成另一个新的样本，该样本服从均值为$\\mu$、方差为$\\frac{\\sigma^2}{M}$的分布（通过简单的概率公式就可以推导出来）。\n",
    "\n",
    "由此，可以对Bagging做出两个结论：\n",
    "1. 样本均值$\\hat{X}$的期望和$X$的期望相等（无偏估计）\n",
    "2. 样本均值$\\hat{X}$的方差比$X$的方差小，且样本数（模型数量）越大，方差就越小。\n",
    "\n",
    "因此Bagging可以降低模型方差，且不改变模型偏差。因此Bagging适用于偏差低、方差高的模型。例如决策树、神经网络。决策树+Bagging就是随机森林了。另外在sklearn中支持对任意的基学习器的Bagging。\n",
    ">- 分类：BaggingClassifier\n",
    ">- 回归：BaggingRegressor\n",
    "\n",
    "Bagging的每个基学习器只在原始数据集的一部分上训练，所以可以不用交叉验证，直接用包外样本上的误差 （out-of-bag error）来估计它的泛化误差/测试误差。\n",
    "<img src=\"./images/Bagging.png\" alt=\"SVM \" style=\"zoom:28%;\" />\n",
    "\n",
    "\n",
    "另外，以上计算是在各个模型之间相互独立的情况下进行的。但是真实情况下并不独立，因为他们会公用许多的数据。所以降低方差的性能会大打折扣。其折扣力度是这么计算的：假设$f_m(x)$之间的相关性是$\\rho$，则$f_{avg}(x)$的方差为$\\rho x \\sigma^2 + (1-\\rho)x\\frac{\\sigma^2}{M}$\n",
    "\n",
    "\n",
    "还有一点，参数基学习器数目n_estimators不是模型复杂度参数，无需通过交叉验证来确定。参数值建议：\n",
    "\n",
    "- 对分类问题，可设置基学习器数目为$\\sqrt{D}$， 其中$D$为特征数目；\n",
    "\n",
    "- 对回归问题，可设置基学习器数目为$\\frac{D}{3}$。\n",
    "\n",
    "#### 1.4 随机森林\n",
    "\n",
    "由于仅训练数据有些不同，对决策树算法进行Bagging得到的多棵树高度相关，因此带来的方差减少有限。\n",
    "\n",
    "随机森林通过以下两个方法来降低树之间的相关性：\n",
    "\n",
    "• 随机选择一部分特征\n",
    "\n",
    "• 随机选择一部分样本\n",
    "\n",
    "\n",
    "随机森林在很多应用案例被证明有效，但牺牲了可解释性\n",
    "\n",
    "• 森林：多棵树\n",
    "\n",
    "• 随机：对样本和特征进行随机抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Boosting\n",
    "\n",
    "学习一个强分类器很难，但是学习一个若分类器很简单。通过学习**一系列**的弱分类器，他们相互结合达到强分类器的效果。需要注意的是，分类器的学习是有顺序的。\n",
    "\n",
    "例如先学习一个弱分类器$\\phi_1$，再学习第二个弱分类器$\\phi_2$。第二个分类器帮助第一个分类器（和第一个分类器互补）。然后再训练第三个弱分类器...一直这么训练下去。\n",
    "\n",
    "最后组合所有M个弱分类器：\n",
    "$$f(x) = \\sum_{m=1}^M\\alpha_m\\phi_m(x)$$\n",
    "\n",
    "有各种各样的Boosting，例如Adaboost、FilterBoost、GentleBoost、GradientBoost、MadaBoost、LogitBoost、LPBoost等等。他们的不同点就是在于**如何设计弱分类器**和**如何把弱分类器结合起来**这两点上。\n",
    "\n",
    "\n",
    "#### 2.1 Adaboost\n",
    "AdaBoost是AdaptiveBoost的缩写，表明该算法是具有适应性的提升算法。\n",
    "\n",
    "算法的步骤如下：\n",
    "\n",
    "1）给每个训练样本（$x_{1},x_{2},….,x_{N}$）分配权重，初始权重$w_{1}$均为1/N。\n",
    "\n",
    "2）针对带有权值的样本进行训练，得到模型$G_m$（初始模型为G1）。\n",
    "\n",
    "3）计算模型$G_m$的误分率$e_m=\\sum_{i=1}^Nw_iI(y_i\\not= G_m(x_i))$\n",
    "\n",
    "4）计算模型$G_m$的系数$\\alpha_m=0.5\\log[(1-e_m)/e_m]$\n",
    "\n",
    "5）根据误分率e和当前权重向量$w_m$更新权重向量$w_{m+1}$。\n",
    "\n",
    "6）计算组合模型$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x_i)$的误分率。\n",
    "\n",
    "7）当组合模型的误分率或迭代次数低于一定阈值，停止迭代；否则，回到步骤2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 产生数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0, 1, -1]])\n",
    "    for i in range(len(data)):\n",
    "        if data[i,-1] == 0:\n",
    "            data[i,-1] = -1\n",
    "    # print(data)\n",
    "    return data[:,:2], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a213ab8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD5CAYAAAA3Os7hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAZ80lEQVR4nO3df4xdZZ3H8fd3h1k7KjIBxlVm6hbFNAqtVkaQdENccbdaa20QsURWq6zsGlwwuBgxBrUxKYZEXSTR8CMrClvsVqzA8mNVbPyxUjMFbNdWIiraGdhlKLbIWrSM3/3j3mmnd+7M3Oeee+55nmc+r2Qyc8995vT7nINfz5zzOeeauyMiIun7s6oLEBGRzlBDFxHJhBq6iEgm1NBFRDKhhi4ikgk1dBGRTBzV6kAz6wFGgDF3X9Xw3jrgKmCsvugad79+tvUdf/zxvmjRoqBiRUTmu+3btz/h7gPN3mu5oQOXALuBF8zw/tfc/YOtrmzRokWMjIwE/PMiImJmv57pvZZOuZjZEPAWYNajbhERqU6r59A/D3wE+NMsY95uZjvMbLOZLWw2wMwuNLMRMxsZHx8PrVVERGYxZ0M3s1XA4+6+fZZhtwOL3H0p8G3gxmaD3P1adx929+GBgaangEREpE2tnENfDqw2s5XAAuAFZnaTu58/OcDd904Zfx3wmc6WKSLSOQcPHmR0dJRnnnmm6lJmtGDBAoaGhujt7W35d+Zs6O5+OXA5gJm9Hvjnqc28vvzF7v5Y/eVqahdPRUSiNDo6ytFHH82iRYsws6rLmcbd2bt3L6Ojo5x44okt/17bOXQzW29mq+svLzazn5rZT4CLgXXtrldEpGzPPPMMxx13XJTNHMDMOO6444L/ggiJLeLuW4Gt9Z+vmLL80FG8SG62PDDGVfc8xKP7DnBCfx+XrVjMmmWDVZclBcXazCe1U19QQxeZb7Y8MMblt+7kwMEJAMb2HeDyW3cCqKlLdHTrv8gsrrrnoUPNfNKBgxNcdc9DFVUkubj77rtZvHgxJ510EldeeWVH1qmGLjKLR/cdCFou0oqJiQkuuugi7rrrLnbt2sXGjRvZtWtX4fXqlIvILE7o72OsSfM+ob+vgmqkKp2+jvLjH/+Yk046iZe+9KUArF27lm9+85u88pWvLFSnjtBFZnHZisX09fYcsayvt4fLViyuqCLptsnrKGP7DuAcvo6y5YGxOX93JmNjYyxcePiG+qGhIcbG2l/fJDV0kVmsWTbIhrOXMNjfhwGD/X1sOHuJLojOI2VcR3H3acs6kbrRKReROaxZNqgGPo+VcR1laGiIPXv2HHo9OjrKCSec0Pb6JukIXURkFjNdLylyHeW1r30tP//5z/nVr37FH//4R2655RZWr1499y/OQQ1dRGQWZVxHOeqoo7jmmmtYsWIFr3jFKzj33HM5+eSTi5aqUy4iIrOZPN3W6buFV65cycqVKztR4iFq6CIic0jlOopOuYiIZEINXUQkE2roIiKZUEMXEcmEGrqISCbU0CUbWx4YY/mV93LiR/+D5VfeW+hZGyJle9/73scLX/hCTjnllI6tUw1dslDGA5REyrRu3Truvvvujq5TDV2yoA+ikFLt2ASfOwU+2V/7vmNT4VWeeeaZHHvssR0o7jDdWCRZ0AdRSGl2bILbL4aD9f+W9u+pvQZYem51dTWhI3TJQhkPUBIB4DvrDzfzSQcP1JZHRg1dsqAPopDS7B8NW14hnXKRLJT1ACURjhmqnWZptjwyauiSjVQeoCSJOeuKI8+hA/T21ZYXcN5557F161aeeOIJhoaG+NSnPsUFF1xQaJ1q6FJYpz9AVyQqkxc+v7O+dprlmKFaMy94QXTjxo0dKO5IauhSyGT+ezIyOJn/BtTUJR9Lz40u0dKMLopKIcp/i8RDDV0KUf5bUuXuVZcwq3bqU0OXQpT/lhQtWLCAvXv3RtvU3Z29e/eyYMGCoN/TOXQp5LIVi484hw7Kf0v8hoaGGB0dZXx8vOpSZrRgwQKGhsKikWroUojy35Ki3t5eTjzxxKrL6Dg1dClM+W+ROLTc0M2sBxgBxtx9VcN7zwG+ApwK7AXe6e6PdLBOkSQoky9VCrkoegmwe4b3LgB+6+4nAZ8DPlO0MJHU6JnsUrWWGrqZDQFvAa6fYcjbgBvrP28GzjIzK16eSDqUyZeqtXqE/nngI8CfZnh/ENgD4O7PAvuB4xoHmdmFZjZiZiMxX10WaYcy+VK1ORu6ma0CHnf37bMNa7JsWsDT3a9192F3Hx4YGAgoUyR+yuRL1Vo5Ql8OrDazR4BbgDeY2U0NY0aBhQBmdhRwDPBkB+sUiZ6eyS5Vm7Ohu/vl7j7k7ouAtcC97n5+w7DbgPfUfz6nPibOW7BESrJm2SAbzl7CYH8fBgz297Hh7CVKuUjXtJ1DN7P1wIi73wbcAHzVzB6mdmS+tkP1iSRFmXypUlBDd/etwNb6z1dMWf4M8I5OFiby8S072bhtDxPu9Jhx3ukL+fSaJVWXJRIt3SkqUfr4lp3cdN9vDr2ecD/0Wk1dpDk9bVGitHFbk89wnGW5iKihS6QmZrimPtNyEVFDl0j1zHCj8UzLRUQNXSJ13ukLg5aLiC6KSqQmL3wq5SLSOqvq/p/h4WEfGRmp5N8WEUmVmW139+Fm7+kIXZp613U/4oe/OPz0huUvO5ab339GhRVVR884l1ToHLpM09jMAX74iyd513U/qqii6ugZ55ISNXSZprGZz7U8Z3rGuaREDV1kFnrGuaREDV1kFnrGuaREDV2mWf6yY4OW50zPOJeUqKHLNDe//4xpzXu+plz0jHNJiXLoIiIJUQ5dgpWVvQ5Zr/LfImHU0GWayez1ZFxvMnsNFGqoIestqwaRnOkcukxTVvY6ZL3Kf4uEU0OXacrKXoesV/lvkXBq6DJNWdnrkPUq/y0STg1dpikrex2yXuW/RcLpoqhMM3nRsdMJk5D1llWDSM6UQxcRSYhy6CWIISMdWkMMNYtIedTQ2xBDRjq0hhhqFpFy6aJoG2LISIfWEEPNIlIuNfQ2xJCRDq0hhppFpFxq6G2IISMdWkMMNYtIudTQ2xBDRjq0hhhqFpFy6aJoG2LISIfWEEPNIlIu5dBFRBJSKIduZguA7wHPqY/f7O6faBizDrgKGKsvusbdry9StHTex7fsZOO2PUy402PGeacv5NNrlhQeG0u+PZY6RKrSyimXPwBvcPenzawX+IGZ3eXu9zWM+5q7f7DzJUonfHzLTm667zeHXk+4H3rd2KhDxsaSb4+lDpEqzXlR1Guerr/srX9Vc55G2rZx256Wl4eMjSXfHksdIlVqKeViZj1m9iDwOPAtd9/WZNjbzWyHmW02s4UzrOdCMxsxs5Hx8fECZUuoiRmulTRbHjI2lnx7LHWIVKmlhu7uE+7+amAIOM3MTmkYcjuwyN2XAt8GbpxhPde6+7C7Dw8MDBSpWwL1mLW8PGRsLPn2WOoQqVJQDt3d9wFbgTc1LN/r7n+ov7wOOLUj1UnHnHd60z+ami4PGRtLvj2WOkSqNGdDN7MBM+uv/9wHvBH4WcOYF095uRrY3ckipbhPr1nC+a97yaGj7B4zzn/dS5omV0LGrlk2yIazlzDY34cBg/19bDh7SdcvRMZSh0iV5syhm9lSaqdQeqj9H8Amd19vZuuBEXe/zcw2UGvkzwJPAh9w95/NuFKUQxcRacdsOXTdWNSmsjLPIfnvMtcdMr8Ut0VydmyC76yH/aNwzBCcdQUsPbfqqqQC+oCLDisr8xyS/y5z3SHzS3FbJGfHJrj9YjhYT+zs31N7DWrqcgQ9nKsNZWWeQ/LfZa47ZH4pbovkfGf94WY+6eCB2nKRKdTQ21BW5jkk/13mukPml+K2SM7+0bDlMm+pobehrMxzSP67zHWHzC/FbZGcY4bClsu8pYbehrIyzyH57zLXHTK/FLdFcs66Anob/g+yt6+2XGQKXRRtQ1nPFp+82FdGsiNk3SHzS3FbJGfywqdSLjIHxRZFRBKi2KIAcWTLJXHKw0dNDX2eiCFbLolTHj56uig6T8SQLZfEKQ8fPTX0eSKGbLkkTnn46KmhzxMxZMslccrDR08NfZ6IIVsuiVMePnq6KDpPxJAtl8QpDx895dBFRBIyr3PoZeWpQ9Yby3O9lS2PTO6Z7tznF6JL2yLrhl5WnjpkvbE811vZ8sjknunOfX4hurgtsr4oWlaeOmS9sTzXW9nyyOSe6c59fiG6uC2ybuhl5alD1hvLc72VLY9M7pnu3OcXoovbIuuGXlaeOmS9sTzXW9nyyOSe6c59fiG6uC2ybuhl5alD1hvLc72VLY9M7pnu3OcXoovbIuuLomXlqUPWG8tzvZUtj0zume7c5xeii9tCOXQRkYTM6xx6WZRvF0nEHZfC9i+DT4D1wKnrYNVni683wpy9GnoblG8XScQdl8LIDYdf+8Th10WaeqQ5+6wvipZF+XaRRGz/ctjyVkWas1dDb4Py7SKJ8Imw5a2KNGevht4G5dtFEmE9YctbFWnOXg29Dcq3iyTi1HVhy1sVac5eF0XboHy7SCImL3x2OuUSac5eOXQRkYQUyqGb2QLge8Bz6uM3u/snGsY8B/gKcCqwF3inuz9SsO6mQvPfqT0DPCRbnvu2KDXnG5JNLquOMucXYUa6Y0LnlvO2aNDKKZc/AG9w96fNrBf4gZnd5e73TRlzAfBbdz/JzNYCnwHe2eliQ/PfqT0DPCRbnvu2KDXnG5JNLquOMucXaUa6I0LnlvO2aGLOi6Je83T9ZW/9q/E8zduAG+s/bwbOMut83CI0/53aM8BDsuW5b4tSc74h2eSy6ihzfpFmpDsidG45b4smWkq5mFmPmT0IPA58y923NQwZBPYAuPuzwH7guCbrudDMRsxsZHx8PLjY0Px3as8AD8mW574tSs35hmSTy6qjzPlFmpHuiNC55bwtmmipobv7hLu/GhgCTjOzUxqGNDsan9aF3P1adx929+GBgYHgYkPz36k9AzwkW577tig15xuSTS6rjjLnF2lGuiNC55bztmgiKIfu7vuArcCbGt4aBRYCmNlRwDHAkx2o7wih+e/UngEeki3PfVuUmvMNySaXVUeZ84s0I90RoXPLeVs00UrKZQA46O77zKwPeCO1i55T3Qa8B/gRcA5wr5eQhwzNf6f2DPCQbHnu26LUnG9INrmsOsqcX6QZ6Y4InVvO26KJOXPoZraU2gXPHmpH9Jvcfb2ZrQdG3P22erTxq8Ayakfma939l7OtVzl0EZFwhXLo7r6DWqNuXH7FlJ+fAd5RpEgRESkm+1v/k7uZRroj5GaTGG5MKfNmmtRunIphf0Qq64ae3M000h0hN5vEcGNKmTfTpHbjVAz7I2JZP20xuZtppDtCbjaJ4caUMm+mSe3GqRj2R8SybujJ3Uwj3RFys0kMN6aUeTNNajdOxbA/IpZ1Q0/uZhrpjpCbTWK4MaXMm2lSu3Eqhv0RsawbenI300h3hNxsEsONKWXeTJPajVMx7I+IZd3Q1ywbZMPZSxjs78OAwf4+Npy9RBdE57ul58Jbr4ZjFgJW+/7Wq5tfVAsZG0O9oePLml9q682EPuBCRCQhhW4sEpn3Qj4MIxap1RxLtjyWOtqkhi4ym5APw4hFajXHki2PpY4Csj6HLlJYyIdhxCK1mmPJlsdSRwFq6CKzCfkwjFikVnMs2fJY6ihADV1kNiEfhhGL1GqOJVseSx0FqKGLzCbkwzBikVrNsWTLY6mjADV0kdms+iwMX3D46NZ6aq9jvLg4KbWaY8mWx1JHAcqhi4gkRDl0KVeK2d2yai4r/53iNpauU0OXYlLM7pZVc1n57xS3sVRC59ClmBSzu2XVXFb+O8VtLJVQQ5diUszullVzWfnvFLexVEINXYpJMbtbVs1l5b9T3MZSCTV0KSbF7G5ZNZeV/05xG0sl1NClmBSzu2XVXFb+O8VtLJVQDl1EJCGz5dB1hC752LEJPncKfLK/9n3Hpu6vt6waRFqgHLrkoaysdsh6lReXiukIXfJQVlY7ZL3Ki0vF1NAlD2VltUPWq7y4VEwNXfJQVlY7ZL3Ki0vF1NAlD2VltUPWq7y4VEwNXfJQVlY7ZL3Ki0vFlEMXEUlIoRy6mS00s++a2W4z+6mZXdJkzOvNbL+ZPVj/0t+YqUsxT628ePm03aLWSg79WeDD7n6/mR0NbDezb7n7roZx33f3VZ0vUbouxTy18uLl03aL3pxH6O7+mLvfX//5d8BuYLDswqRCKeaplRcvn7Zb9IIuiprZImAZsK3J22eY2U/M7C4zO3mG37/QzEbMbGR8fDy4WOmSFPPUyouXT9stei03dDN7PvB14EPu/lTD2/cDf+nurwK+AGxptg53v9bdh919eGBgoN2apWwp5qmVFy+ftlv0WmroZtZLrZnf7O63Nr7v7k+5+9P1n+8Ees3s+I5WKt2TYp5aefHyabtFr5WUiwE3ALvdvemDnc3sRfVxmNlp9fXu7WSh0kUp5qmVFy+ftlv05syhm9lfAd8HdgJ/qi/+GPASAHf/kpl9EPgAtUTMAeBSd/+v2darHLqISLjZcuhzxhbd/QeAzTHmGuCa9sqTtu3YVEsY7B+tncc864r5fbR0x6Ww/cu1D2W2ntpHvxX9tCCRhOh56KlSJvhId1wKIzccfu0Th1+rqcs8oWe5pEqZ4CNt/3LYcpEMqaGnSpngI/lE2HKRDKmhp0qZ4CNZT9hykQypoadKmeAjnboubLlIhtTQU6VM8JFWfRaGLzh8RG49tde6ICrziJ6HLiKSkEI59PlkywNjXHXPQzy67wAn9Pdx2YrFrFmW0YMlc8+t5z6/GGgbR00NvW7LA2NcfutODhyspSLG9h3g8lt3AuTR1HPPrec+vxhoG0dP59DrrrrnoUPNfNKBgxNcdc9DFVXUYbnn1nOfXwy0jaOnhl736L4DQcuTk3tuPff5xUDbOHpq6HUn9PcFLU9O7rn13OcXA23j6Kmh1122YjF9vUfehNLX28NlKxZXVFGH5Z5bz31+MdA2jp4uitZNXvjMNuUyedEq14RC7vOLgbZx9JRDFxFJyGw5dJ1yEUnBjk3wuVPgk/217zs2pbFu6SqdchGJXZn5b2XLs6IjdJHYlZn/VrY8K2roIrErM/+tbHlW1NBFYldm/lvZ8qyooYvErsz8t7LlWVFDF4ldmc++13P1s6IcuohIQpRDFxGZB9TQRUQyoYYuIpIJNXQRkUyooYuIZEINXUQkE2roIiKZUEMXEcnEnA3dzBaa2XfNbLeZ/dTMLmkyxszsajN72Mx2mNlryilXCtFzr0Wy1srz0J8FPuzu95vZ0cB2M/uWu++aMubNwMvrX6cDX6x/l1joudci2ZvzCN3dH3P3++s//w7YDTR+0ObbgK94zX1Av5m9uOPVSvv03GuR7AWdQzezRcAyYFvDW4PAnimvR5ne9DGzC81sxMxGxsfHwyqVYvTca5HstdzQzez5wNeBD7n7U41vN/mVaU/9cvdr3X3Y3YcHBgbCKpVi9Nxrkey11NDNrJdaM7/Z3W9tMmQUWDjl9RDwaPHypGP03GuR7LWScjHgBmC3u392hmG3Ae+up11eB+x398c6WKcUpedei2SvlZTLcuDvgJ1m9mB92ceAlwC4+5eAO4GVwMPA74H3dr5UKWzpuWrgIhmbs6G7+w9ofo586hgHLupUUSIiEk53ioqIZEINXUQkE2roIiKZUEMXEcmEGrqISCbU0EVEMqGGLiKSCatFyCv4h83GgV9X8o/P7XjgiaqLKJHml66c5waaXyv+0t2bPgyrsoYeMzMbcffhqusoi+aXrpznBppfUTrlIiKSCTV0EZFMqKE3d23VBZRM80tXznMDza8QnUMXEcmEjtBFRDKhhi4ikol53dDNrMfMHjCzO5q8t87Mxs3swfrX31dRYxFm9oiZ7azXP9LkfTOzq83sYTPbYWavqaLOdrQwt9eb2f4p+y+pz9ozs34z22xmPzOz3WZ2RsP7ye47aGl+ye4/M1s8pe4HzewpM/tQw5hS9l8rn1iUs0uA3cALZnj/a+7+wS7WU4a/dveZbmR4M/Dy+tfpwBfr31Mx29wAvu/uq7pWTWf9C3C3u59jZn8OPLfh/dT33Vzzg0T3n7s/BLwaageNwBjwjYZhpey/eXuEbmZDwFuA66uupUJvA77iNfcB/Wb24qqLmu/M7AXAmdQ+yxd3/6O772sYluy+a3F+uTgL+IW7N94VX8r+m7cNHfg88BHgT7OMeXv9z6HNZrawS3V1kgP/aWbbzezCJu8PAnumvB6tL0vBXHMDOMPMfmJmd5nZyd0srqCXAuPAv9ZPCV5vZs9rGJPyvmtlfpDu/ptqLbCxyfJS9t+8bOhmtgp43N23zzLsdmCRuy8Fvg3c2JXiOmu5u7+G2p93F5nZmQ3vN/us2FRyrHPN7X5qz7x4FfAFYEu3CyzgKOA1wBfdfRnwf8BHG8akvO9amV/K+w+A+qmk1cC/N3u7ybLC+29eNnRgObDazB4BbgHeYGY3TR3g7nvd/Q/1l9cBp3a3xOLc/dH698epncM7rWHIKDD1L48h4NHuVFfMXHNz96fc/en6z3cCvWZ2fNcLbc8oMOru2+qvN1NrgI1jktx3tDC/xPffpDcD97v7/zZ5r5T9Ny8burtf7u5D7r6I2p9E97r7+VPHNJzPWk3t4mkyzOx5Znb05M/A3wL/3TDsNuDd9SvurwP2u/tjXS41WCtzM7MXmZnVfz6N2n/re7tdazvc/X+APWa2uL7oLGBXw7Ak9x20Nr+U998U59H8dAuUtP/me8rlCGa2Hhhx99uAi81sNfAs8CSwrsra2vAXwDfq/5s4Cvg3d7/bzP4RwN2/BNwJrAQeBn4PvLeiWkO1MrdzgA+Y2bPAAWCtp3Vb9D8BN9f/bP8l8N5M9t2kueaX9P4zs+cCfwP8w5Rlpe8/3fovIpKJeXnKRUQkR2roIiKZUEMXEcmEGrqISCbU0EVEMqGGLiKSCTV0EZFM/D/9eJgL33QQQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:50,0],X[:50,1], label='0')\n",
    "plt.scatter(X[50:,0],X[50:,1], label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50, learning_rate=1.0):\n",
    "        self.clf_num = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def init_args(self, datasets, labels):\n",
    "\n",
    "        self.X = datasets\n",
    "        self.Y = labels\n",
    "        self.M, self.N = datasets.shape\n",
    "\n",
    "        # 弱分类器数目和集合\n",
    "        self.clf_sets = []\n",
    "\n",
    "        # 初始化weights\n",
    "        self.weights = [1.0 / self.M] * self.M\n",
    "\n",
    "        # G(x)系数 alpha\n",
    "        self.alpha = []\n",
    "\n",
    "    def _G(self, features, labels, weights):\n",
    "        m = len(features)\n",
    "        error = 100000.0  # 无穷大\n",
    "        best_v = 0.0\n",
    "        # 单维features\n",
    "        features_min = min(features)\n",
    "        features_max = max(features)\n",
    "        n_step = (features_max - features_min +\n",
    "                  self.learning_rate) // self.learning_rate\n",
    "        # print('n_step:{}'.format(n_step))\n",
    "        direct, compare_array = None, None\n",
    "        for i in range(1, int(n_step)):\n",
    "            v = features_min + self.learning_rate * i\n",
    "\n",
    "            if v not in features:\n",
    "                # 误分类计算\n",
    "                compare_array_positive = np.array(\n",
    "                    [1 if features[k] > v else -1 for k in range(m)])\n",
    "                weight_error_positive = sum([\n",
    "                    weights[k] for k in range(m)\n",
    "                    if compare_array_positive[k] != labels[k]\n",
    "                ])\n",
    "\n",
    "                compare_array_nagetive = np.array(\n",
    "                    [-1 if features[k] > v else 1 for k in range(m)])\n",
    "                weight_error_nagetive = sum([\n",
    "                    weights[k] for k in range(m)\n",
    "                    if compare_array_nagetive[k] != labels[k]\n",
    "                ])\n",
    "\n",
    "                if weight_error_positive < weight_error_nagetive:\n",
    "                    weight_error = weight_error_positive\n",
    "                    _compare_array = compare_array_positive\n",
    "                    direct = 'positive'\n",
    "                else:\n",
    "                    weight_error = weight_error_nagetive\n",
    "                    _compare_array = compare_array_nagetive\n",
    "                    direct = 'nagetive'\n",
    "\n",
    "                # print('v:{} error:{}'.format(v, weight_error))\n",
    "                if weight_error < error:\n",
    "                    error = weight_error\n",
    "                    compare_array = _compare_array\n",
    "                    best_v = v\n",
    "        return best_v, direct, error, compare_array\n",
    "\n",
    "    # 计算alpha\n",
    "    def _alpha(self, error):\n",
    "        return 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "    # 规范化因子\n",
    "    def _Z(self, weights, a, clf):\n",
    "        return sum([\n",
    "            weights[i] * np.exp(-1 * a * self.Y[i] * clf[i])\n",
    "            for i in range(self.M)\n",
    "        ])\n",
    "\n",
    "    # 权值更新\n",
    "    def _w(self, a, clf, Z):\n",
    "        for i in range(self.M):\n",
    "            self.weights[i] = self.weights[i] * np.exp(\n",
    "                -1 * a * self.Y[i] * clf[i]) / Z\n",
    "\n",
    "    # G(x)的线性组合\n",
    "    def _f(self, alpha, clf_sets):\n",
    "        pass\n",
    "\n",
    "    def G(self, x, v, direct):\n",
    "        if direct == 'positive':\n",
    "            return 1 if x > v else -1\n",
    "        else:\n",
    "            return -1 if x > v else 1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.init_args(X, y)\n",
    "\n",
    "        for epoch in range(self.clf_num):\n",
    "            best_clf_error, best_v, clf_result = 100000, None, None\n",
    "            # 根据特征维度, 选择误差最小的\n",
    "            for j in range(self.N):\n",
    "                features = self.X[:, j]\n",
    "                # 分类阈值，分类误差，分类结果\n",
    "                v, direct, error, compare_array = self._G(\n",
    "                    features, self.Y, self.weights)\n",
    "\n",
    "                if error < best_clf_error:\n",
    "                    best_clf_error = error\n",
    "                    best_v = v\n",
    "                    final_direct = direct\n",
    "                    clf_result = compare_array\n",
    "                    axis = j\n",
    "\n",
    "                # print('epoch:{}/{} feature:{} error:{} v:{}'.format(epoch, self.clf_num, j, error, best_v))\n",
    "                if best_clf_error == 0:\n",
    "                    break\n",
    "\n",
    "            # 计算G(x)系数a\n",
    "            a = self._alpha(best_clf_error)\n",
    "            self.alpha.append(a)\n",
    "            # 记录分类器\n",
    "            self.clf_sets.append((axis, best_v, final_direct))\n",
    "            # 规范化因子\n",
    "            Z = self._Z(self.weights, a, clf_result)\n",
    "            # 权值更新\n",
    "            self._w(a, clf_result, Z)\n",
    "\n",
    "\n",
    "#             print('classifier:{}/{} error:{:.3f} v:{} direct:{} a:{:.5f}'.format(epoch+1, self.clf_num, error, best_v, final_direct, a))\n",
    "#             print('weight:{}'.format(self.weights))\n",
    "#             print('\\n')\n",
    "\n",
    "    def predict(self, feature):\n",
    "        result = 0.0\n",
    "        for i in range(len(self.clf_sets)):\n",
    "            axis, clf_v, direct = self.clf_sets[i]\n",
    "            f_input = feature[axis]\n",
    "            result += self.alpha[i] * self.G(f_input, clf_v, direct)\n",
    "        # sign\n",
    "        return 1 if result > 0 else -1\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        right_count = 0\n",
    "        for i in range(len(X_test)):\n",
    "            feature = X_test[i]\n",
    "            if self.predict(feature) == y_test[i]:\n",
    "                right_count += 1\n",
    "\n",
    "        return right_count / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575757575757576"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(10).reshape(10, 1)\n",
    "y = np.array([1, 1, 1, -1, -1, -1, 1, 1, 1, -1])\n",
    "\n",
    "clf = AdaBoost(n_estimators=3, learning_rate=0.5)\n",
    "clf.fit(X, y)\n",
    "\n",
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "clf = AdaBoost(n_estimators=10, learning_rate=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score:67.576%\n"
     ]
    }
   ],
   "source": [
    "# 100次结果\n",
    "result = []\n",
    "for i in range(1, 101):\n",
    "    X, y = create_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    clf = AdaBoost(n_estimators=100, learning_rate=0.2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    r = clf.score(X_test, y_test)\n",
    "    # print('{}/100 score：{}'.format(i, r))\n",
    "    result.append(r)\n",
    "\n",
    "print('average score:{:.3f}%'.format(sum(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implements with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "- algorithm：这个参数只有AdaBoostClassifier有。主要原因是scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用了和我们的原理篇里二元分类Adaboost算法的扩展，即用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。\n",
    "\n",
    "\n",
    "- n_estimators： AdaBoostClassifier和AdaBoostRegressor都有，就是我们的弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n",
    "\n",
    "\n",
    "-  learning_rate:  AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重缩减系数ν\n",
    "\n",
    "\n",
    "- base_estimator：AdaBoostClassifier和AdaBoostRegressor都有，即我们的弱分类学习器或者弱回归学习器。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.5,\n",
       "                   n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8787878787878788"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
